
# Step 1 : Creating a S3 Bucket and uploading the Files

- I will create a Bucket in the Ohio Region (since it has some advanced AI models that other regions don't have).

**Bucket Type** : General purpose
```text
research-document-storage
```
Rest of them I kept as it as the Default settings.

# Step 2 : Creating a Knowledge Base 

## Giving the Knowledge base a name

```text
research-document-knowledge
```
IAM Permission -> Create a New service role
Data source -> Amazon S3

## Give the Data source a name,

```text
s3-research-document
```
## Select the S3 Bucket

Search and select the S3 bucket that we created in the first step

```text
research-document-storage
```

 Using the Default Parsing and Chunking strategy

 ## Embedding Model

 In the Ohio region there is as of right now only one Embedding Model that is the Titan Text Embedding Model.

An Embedding model converts text into numerical representations called vectors, where the numbers capture the meaning and relationships between words.

## Default Vector Database

Iam creating a new vector store for this project since i can't use any existing ones. I am using **Amazon OpenSearch Serverless** , this will store all the numbers that are generated by the Embedding model.

# Step 3 : Accessing the AI Models on Amazon Bedrock

I am going to use **Llama 3.3 70B instruct model** along with the already mentioned **Titan Text Embedding model**.

## How they work together: 

### ğŸ—ï¸ 1. Ingestion / Indexing
- ğŸ“„ **Input:** Knowledge base documents
- âš™ï¸ **Process:** 
  - Documents are converted into **vector embeddings** using **Titan Text Embedding Model**
- ğŸ§© **Output:** 
  - Embeddings (vectors) stored in a **Vector Database** (e.g., Pinecone, FAISS, or OpenSearch)

---

### ğŸ” 2. Retrieval (Semantic Search)
- â“ **Input:** Userâ€™s query/question
- âš™ï¸ **Process:**
  - Query is **embedded** using the same **Titan Text Embedding Model**
  - Perform **semantic similarity search** against stored vectors in the Vector Database
- ğŸ“‘ **Output:** 
  - Retrieve **most relevant document chunks**

---

### ğŸ§  3. Generation (LLM)
- ğŸ§¾ **Input:** Retrieved text snippets (context)
- âš™ï¸ **Model Used:** **Llama 3.3 70B Instruct**
- ğŸ”§ **Process:** 
  - Combine **user query + retrieved context**
  - Generate a coherent, context-aware response

---

### ğŸ’¬ 4. Response
- ğŸ§© **Model Behavior:** 
  - Llama model follows instruction tuning
  - Leverages retrieved knowledge context
- ğŸ¯ **Output:** 
  - **Accurate, context-rich answer** to userâ€™s question

__Check once if you have access to both the Models__

 ## Sync the Datasource and Knowledge Base

 After the Knowledge Base and the Data source are created we have to sync them.
 So go to the Knowledge Base and then scroll down I found the Data source named

```text
 s3-research-document
```

Syncing is the process in which the Data stored in the Data source gets poured into the Knowledge Base.

**If we want to add some more new information then add those data into the S3 bucket and then sync again**

# Step 4 : Using AWS CloudShell to run Terminal commands
